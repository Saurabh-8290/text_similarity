{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaefba97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer  # For Lemmetization of words\n",
    "from nltk.corpus import stopwords  # Load list of stopwords\n",
    "from nltk import word_tokenize # Convert paragraph in tokens\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aafc612a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('Precily_Text_Similarity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25e40820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1982ba6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text1    0\n",
       "text2    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "234ebab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbbed7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 3000/3000 [47:51<00:00,  1.04it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 3000/3000 [28:15<00:00,  1.77it/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text_column):\n",
    "    preprocessed_text = []\n",
    "    for sentence in tqdm(text_column.values):\n",
    "        sent = re.sub('[^A-Za-z0-9]+', ' ', sentence)\n",
    "        sent = ' '.join(e for e in sent.split() if e not in stopwords.words('english'))\n",
    "        preprocessed_text.append(sent.lower().strip())\n",
    "    return preprocessed_text\n",
    "\n",
    "# Example usage\n",
    "df1['text1'] = preprocess_text(df1['text1'])\n",
    "df1['text2'] = preprocess_text(df1['text2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b67e50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenizer(text):\n",
    "    #tokenizes and stems the text\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1601950",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 3000/3000 [00:15<00:00, 194.45it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(df1.index):\n",
    "    s1=df1['text1'][i]\n",
    "    s2=df1['text2'][i]\n",
    "    s1words = ' '.join(word_tokenizer(s1))\n",
    "    s2words = ' '.join(word_tokenizer(s2))\n",
    "    df1['text1'][i]=s1words\n",
    "    df1['text2'][i]=s2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f72cc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb64c7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(row):\n",
    "    tfidf_matrix = tfidf.fit_transform(row[['text1', 'text2']].values.astype('U'))\n",
    "    cos_sim=cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])\n",
    "    return cos_sim[0][0]\n",
    "df1['similarity'] = df1.apply(calculate_similarity, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "199b7272",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['binary_score'] = (df1['similarity'] > 0.5).astype(int)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
